# -*- coding: utf-8 -*-
"""RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hb9CANjLG525TNcVYNRTIJU2pEDO4npO
"""
import argparse

# !chmod 777 download.sh

# !./download.sh

# !pip install keras_preprocessing
import utils

import torch
import torch.nn as nn
import torch.optim as optim

MODEL_NAMES = ["RNN", "LSTM", "Transformer"]


class RNNClassifier(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, num_layers, vocab_size, bidirectional):
        super(RNNClassifier, self).__init__()
        self.hidden_dim = hidden_dim
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)
        self.bidirectional = bidirectional

        self.rnn = nn.RNN(embedding_dim, hidden_dim, num_layers=num_layers, bidirectional=self.bidirectional)

        if self.bidirectional:
            self.ln = nn.LayerNorm(hidden_dim * 2)

            # The linear layer that maps from hidden state space to tag space
            self.hidden2label = nn.Linear(hidden_dim * 2, 1)
        else:
            self.ln = nn.LayerNorm(hidden_dim)

            # The linear layer that maps from hidden state space to tag space
            self.hidden2label = nn.Linear(hidden_dim, 1)

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence.permute(1, 0))
        rnn_out, hidden = self.rnn(embeds)
        rnn_out = self.ln(rnn_out)
        outputs = rnn_out[-1]
        label_space = self.hidden2label(outputs)
        label = torch.sigmoid(label_space)
        return label


class LSTMClassifier(nn.Module):

    def __init__(self, embedding_dim, hidden_dim, vocab_size, num_layers, bidirectional):
        super(LSTMClassifier, self).__init__()
        self.hidden_dim = hidden_dim
        self.bidirectional = bidirectional
        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)

        # The LSTM takes word embeddings as inputs, and outputs hidden states
        # with dimensionality hidden_dim.
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, bidirectional=self.bidirectional)

        if self.bidirectional:
            self.ln = nn.LayerNorm(hidden_dim * 2)

            # The linear layer that maps from hidden state space to tag space
            self.hidden2label = nn.Linear(hidden_dim * 2, 1)
        else:
            self.ln = nn.LayerNorm(hidden_dim)

            # The linear layer that maps from hidden state space to tag space
            self.hidden2label = nn.Linear(hidden_dim, 1)

    def forward(self, sentence):
        embeds = self.word_embeddings(sentence.permute(1, 0))
        lstm_out, _ = self.lstm(embeds)
        lstm_out = self.ln(lstm_out)
        outputs = lstm_out[-1]
        label_space = self.hidden2label(outputs)
        label = torch.sigmoid(label_space)
        return label


def train(args):
    print("Loading Vocabulary......")
    vocab_set = utils.load_vocab()
    vocab_set.add('<UNK>')
    # vocab_set.add('<PAD>')
    words_set = sorted(vocab_set)
    words_index = {x: i + 1 for i, x in enumerate(vocab_set)}
    vocab_set.add('<PAD>')
    words_index['<PAD>'] = 0

    print("Loading dataset.......")
    train_sentences, train_labels = utils.load_data('train', vocab_set)
    test_sentences, test_labels = utils.load_data('test', vocab_set)

    print(len(train_sentences), len(train_labels), len(test_sentences), len(test_labels))

    print("Generate train and test dataframe......")
    import random
    df_train = []
    for i in range(len(train_sentences)):
        df_train.append((train_sentences[i], train_labels[i]))
    random.shuffle(df_train)

    df_test = []
    for i in range(len(test_sentences)):
        df_test.append((test_sentences[i], test_labels[i]))

    # visualization using tensorboard
    from torch.utils.tensorboard import SummaryWriter
    writer = SummaryWriter('runs/experiment_1')

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    # set seed
    if device == torch.device('cuda'):
        torch.cuda.manual_seed(3407)
    else:
        torch.manual_seed(3407)

    if args.model_name == "RNN":
        print("The model used is " + args.model_name)
        model = RNNClassifier(args.embedding_dim, args.hidden_dim, args.num_layers, len(vocab_set), args.bidirectional)
    elif args.model_name == "LSTM":
        print("The model used is " + args.model_name)
        model = LSTMClassifier(args.embedding_dim, args.hidden_dim, len(vocab_set), args.num_layers, args.bidirectional)
    model.to(device)

    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)
    # scheduler = lr_scheduler.CosineAnnealingLR(optimizer, 10)
    loss_function = nn.BCELoss()

    print("Start Training")
    for epoch in range(args.epochs):
        running_loss = 0.
        total_loss = 0.
        model.train()
        for i, (sentences, label) in enumerate(utils.generate_batch(df_train, args.batch_size)):
            model.zero_grad()

            sentence_in = utils.prepare_sequences(sentences, words_index).to(device)
            targets = torch.tensor(label, dtype=torch.float).to(device)

            # forward
            y_pred = model(sentence_in)
            # compute loss
            loss = loss_function(y_pred.view(-1), targets)
            loss.backward()
            optimizer.step()
            # scheduler.step()

            running_loss += loss.item()
            total_loss += loss.item()

        print ('[%d] loss: %.3f' % (epoch + 1, total_loss / (i + 1)))
        writer.add_scalar(args.model_name + 'Loss--lr=' + str(args.learning_rate)
                            + 'num_layers=' + str(args.num_layers),
                            total_loss / (i+1), epoch + 1)
        predictions = utils.predict(model, df_test, device, words_index)
        acc, precision, recall, f1 = utils.compute_metrics(predictions, test_labels)
        writer.add_scalar(args.model_name + 'Acc--lr=' + str(args.learning_rate)
                            + 'num_layers=' + str(args.num_layers),
                            acc, epoch + 1)
        writer.add_scalar(args.model_name + 'prec--lr=' + str(args.learning_rate)
                            + 'num_layers=' + str(args.num_layers),
                            precision, epoch + 1)
        writer.add_scalar(args.model_name + 'recall--lr=' + str(args.learning_rate)
                            + 'num_layers=' + str(args.num_layers),
                            recall, epoch + 1)
        writer.add_scalar(args.model_name + 'f1--lr=' + str(args.learning_rate)
                            + 'num_layers=' + str(args.num_layers),
                            f1, epoch + 1)
        print ("Evaluation Result: [Accuracy = %.4f, Precision = %.4f, Recall = %.4f, F1 = %.4f]" % (acc, precision, recall, f1))
        torch.cuda.empty_cache()
    return model

# LEARNING_RATE = [0.00005, 0.0001, 0.0005]
# BATCH_SIZE = 32
# EMBEDDING_DIM = 512
# HIDDEN_DIM = 64
# NUM_LAYERS = [1, 3]
# EPOCHS = 50
#
# for lr in LEARNING_RATE:
#     for num_layers in NUM_LAYERS:
#         print ("---------------------------------------Train Start------------------------------------------")
#         print ("-------------Learning rate = " + str(lr) + " number of layers = " + str(num_layers) + "---------------")
#         trainRNN(lr, EPOCHS, EMBEDDING_DIM, HIDDEN_DIM, num_layers, vocab_set)

# model = trainRNN(0.0005, 50, 512, 32, 3, vocab_set)

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir=runs/experiment_1


def main():
    parser = argparse.ArgumentParser()

    parser.add_argument("--model_name", default=None, type=str, required=True,
                        help="The model_name should be RNN, LSTM or Transformer")
    parser.add_argument("--learning_rate", default=5e-5, type=float,
                        help="The learning rate of optimizer")
    parser.add_argument("--batch_size", default=32, type=int,
                        help="The batch size")
    parser.add_argument("--num_layers", default=1, type=int,
                        help="The number of layers of stacked RNN or LSTM")
    parser.add_argument("--embedding_dim", default=512, type=int,
                        help="The word embedding dimension")
    parser.add_argument("--epochs", default=20, type=int,
                        help="The number of epochs")
    parser.add_argument("--hidden_dim", default=64, type=int,
                        help="The hidden layer dimension")
    parser.add_argument("--bidirectional", default=True, type=bool,
                        help="Bidirectional RNN or bidirectional LSTM  ")
    args = parser.parse_args()

    if args.model_name not in MODEL_NAMES:
        raise ValueError("The model name should be one of RNN, LSTM or Transformer")

    train(args)


if __name__ == '__main__':
    main()
